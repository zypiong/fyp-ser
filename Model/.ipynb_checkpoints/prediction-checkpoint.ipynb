{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e676a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import moviepy.editor as mp\n",
    "import subprocess\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "from time import strftime, gmtime\n",
    "\n",
    "from config import CHECKPOINTS_DIR_PATH\n",
    "from config import VIDEOS_FILE_PATH\n",
<<<<<<< HEAD
    "from config import AUDIO_ONLY_PATH\n",
    "from config import DATASET_DIR_PATH"
=======
    "from config import AUDIO_ONLY_PATH"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a45577",
   "metadata": {},
   "source": [
    "# Process video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c3f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = VIDEOS_FILE_PATH + 'video1.mp4'\n",
    "video = mp.VideoFileClip(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0fbd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "MoviePy - Writing audio in C:\\Users\\ZiyaoPiong\\Desktop\\fyp-ser\\AudioOnly\\audio1.wav\n"
=======
      "MoviePy - Writing audio in /Users/zypiong/Desktop/fyp-ser/AudioOnly/audio1.wav\n"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "                                                                                                                       "
=======
      "                                                                        "
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "des = AUDIO_ONLY_PATH + 'audio1.wav'\n",
    "video.audio.write_audiofile(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a98c386",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "AUDIO_CHUNKS_PATH = AUDIO_ONLY_PATH + 'audio_chunks\\\\'\n",
=======
    "AUDIO_CHUNKS_PATH = AUDIO_ONLY_PATH + 'audio_chunks/'\n",
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
    "\n",
    "class SplitWavAudio():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
<<<<<<< HEAD
    "        self.filepath = folder + '\\\\' + filename\n",
=======
    "        self.filepath = folder + '/' + filename\n",
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def single_split(self, from_sec, to_sec):\n",
    "        t1 = from_sec * 1000\n",
    "        t2 = to_sec * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
<<<<<<< HEAD
    "        filename = strftime(\"%H-%M-%S\", gmtime(from_sec)) + '_' + strftime(\"%H-%M-%S\", gmtime(to_sec)) + '.wav'\n",
=======
    "        filename = strftime(\"%H:%M:%S\", gmtime(from_sec)) + '_' + strftime(\"%H:%M:%S\", gmtime(to_sec)) + '.wav'\n",
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
    "        path = AUDIO_CHUNKS_PATH + filename\n",
    "        split_audio.export(path, format='wav')\n",
    "        \n",
    "    def multiple_split(self, sec_per_split):\n",
    "        total_secs = int(self.audio.duration_seconds)\n",
    "        for i in range(0, total_secs, sec_per_split):\n",
    "            self.single_split(i, i+sec_per_split)\n",
    "#             print(str(i) + ' to ' + str(i+sec_per_split) + ' done')\n",
    "            if i + sec_per_split > total_secs:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3243f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splited successfully\n"
     ]
    }
   ],
   "source": [
    "split_wav = SplitWavAudio(AUDIO_ONLY_PATH, 'audio1.wav')\n",
    "split_wav.multiple_split(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb91348",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "\n",
    "def get_waveforms(path):\n",
    "    \"\"\"\n",
    "    load all audio file from path\n",
    "    load full 5 seconds of the audio file; nativve sample rate = 48k\n",
    "    \"\"\"\n",
    "    waveforms = [] # waveforms to augment later\n",
    "    file_count = 0\n",
    "    \n",
    "    for root, subdr, files in os.walk(path):\n",
    "        for file in files:\n",
    "            X, sample_rate = librosa.load(os.path.join(root, file), duration=3, res_type='kaiser_fast', sr=SAMPLE_RATE)\n",
    "\n",
    "            waveform = np.zeros((SAMPLE_RATE*3,))\n",
    "            waveform[:len(X)] = X\n",
    "\n",
    "            waveforms.append(waveform)\n",
    "\n",
    "            file_count += 1\n",
    "            # keep track of data loader's progress\n",
    "            print('\\r'+f' Processed {file_count} audio samples',end='')\n",
    "\n",
    "    return waveforms\n",
    "\n",
    "\n",
    "def get_features(waveforms):\n",
    "    features = []\n",
    "    file_count = 0\n",
    "    \n",
    "    for waveform in waveforms:\n",
    "        mfccs = librosa.feature.mfcc(waveform,\n",
    "                                     sr=SAMPLE_RATE,\n",
    "                                     n_mfcc=40,\n",
    "                                     n_fft=1024,\n",
    "                                     win_length=512,\n",
    "                                     window='hamming',\n",
    "                                     n_mels=128,\n",
    "                                     fmax=SAMPLE_RATE/2)\n",
    "        features.append(mfccs)\n",
    "        file_count += 1\n",
    "        # print progress \n",
    "        print('\\r'+f' Processed {file_count} waveforms',end='')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 10,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "5cd7fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed 517 audio samples"
     ]
    }
   ],
   "source": [
    "waveforms = get_waveforms(AUDIO_CHUNKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 11,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "b2b09bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed 517 waveforms"
     ]
    }
   ],
   "source": [
    "features = get_features(waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3249f",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 12,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "136902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parallel_all_you_want(nn.Module):\n",
    "    # Define all layers present in the network\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__() \n",
    "        \n",
    "        ################ TRANSFORMER BLOCK #############################\n",
    "        # maxpool the input feature map/tensor to the transformer \n",
    "        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )\n",
    "        \n",
    "        # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper\n",
    "        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
    "        \n",
    "        ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock1 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock2 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        ################# FINAL LINEAR BLOCK ####################\n",
    "        # Linear softmax layer to take final concatenated embedding tensor \n",
    "        #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
    "        # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
    "        # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
    "        # 512*2+40 == 1064 input features --> 8 output emotions \n",
    "        self.fc1_linear = nn.Linear(512*2+40,num_emotions) \n",
    "        \n",
    "        ### Softmax layer for the 8 output logits from final FC linear layer \n",
    "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
    "        \n",
    "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
    "    def forward(self,x):\n",
    "        \n",
    "        ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
    "        # create final feature embedding from 1st convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
    "        \n",
    "        ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
    "        # create final feature embedding from 2nd convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
    "        \n",
    "         \n",
    "        ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
    "        # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
    "        x_maxpool = self.transformer_maxpool(x)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x = x_maxpool_reduced.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        \n",
    "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
    "        \n",
    "        ############# concatenate freq embeddings from convolutional and transformer blocks ######\n",
    "        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks\n",
    "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
    "\n",
    "        ######### final FC linear layer, need logits for loss #########################\n",
    "        output_logits = self.fc1_linear(complete_embedding)  \n",
    "        \n",
    "        ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
    "        output_softmax = self.softmax_out(output_logits)\n",
    "        \n",
    "        # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
    "        return output_logits, output_softmax"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 13,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "b5b3c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict ={\n",
    "    '0':'neutral',\n",
    "    '1':'calm',\n",
    "    '2':'happy',\n",
    "    '3':'sad',\n",
    "    '4':'angry',\n",
    "    '5':'fearful',\n",
    "    '6':'disgust',\n",
    "    '7':'surprised'\n",
    "}\n",
    "\n",
    "# need device to instantiate model\n",
    "device = 'cpu'\n",
    "\n",
    "# instantiate model for 8 emotions and move to GPU \n",
    "model = parallel_all_you_want(len(emotions_dict)).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# define loss function; CrossEntropyLoss() fairly standard for multiclass problems \n",
    "def criterion(predictions, targets): \n",
    "    return nn.CrossEntropyLoss()(input=predictions, target=targets)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 14,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "c6fb5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(optimizer, model, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    model.load_state_dict(checkpoint_dict['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 15,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "68d5b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Loaded model from C:\\Users\\ZiyaoPiong\\Desktop\\fyp-ser\\Model\\checkpoints\\parallel_all_you_wantFINAL-016.pkl\n"
=======
      "Loaded model from /Users/zypiong/Desktop/fyp-ser/Model/checkpoints/parallel_all_you_wantFINAL-016.pkl\n"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
     ]
    }
   ],
   "source": [
    "# pick load folder  \n",
    "load_folder = CHECKPOINTS_DIR_PATH  \n",
    "\n",
    "# pick the epoch to load\n",
    "epoch = '016'\n",
    "model_name = f'parallel_all_you_wantFINAL-{epoch}.pkl'\n",
    "\n",
    "# make full load path\n",
    "load_path = os.path.join(load_folder, model_name)\n",
    "\n",
    "## instantiate empty model and populate with params from binary \n",
    "model = parallel_all_you_want(len(emotions_dict))\n",
    "load_checkpoint(optimizer, model, load_path)\n",
    "\n",
    "print(f'Loaded model from {load_path}')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 77,
=======
   "execution_count": 16,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "eb258e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
<<<<<<< HEAD
    "    return predictions\n",
    "\n",
    "def get_emotion_counts(predictions):\n",
    "    array = [0] * len(emotions_dict)\n",
    "    for label in predictions:\n",
    "        array[label] += 1\n",
    "    return array\n",
    "\n",
    "def get_timestamp_counts(predictions):\n",
    "    emotion =[[] for _ in range(len(emotions_dict))]\n",
    "    prev = None\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != prev:\n",
    "            emotion[predictions[i]].append(i)\n",
    "            prev = predictions[i]\n",
    "\n",
    "    timestamp_counts = []\n",
    "    for i in range(len(emotion)):\n",
    "        timestamp_counts.append(len(emotion[i]))\n",
    "    return timestamp_counts, sum(timestamp_counts)"
=======
    "    return predictions"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
   "id": "fd842d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(features,device=device).float()\n",
    "# Y = torch.tensor(y_train[:1],dtype=torch.long,device=device)\n",
    "print(predict(X_test_tensor))\n",
    "print(y_test[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "47cfc52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 40, 282)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 14,
=======
     "execution_count": 18,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 20,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "id": "ada028fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 1, 40, 282)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 20,
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.expand_dims(features,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "id": "bd41a36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train scaled:(517, 1, 40, 282)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#### Scale the data ####\n",
    "# store shape so we can transform it back \n",
    "N,C,H,W = X.shape\n",
    "# Reshape to 1D because StandardScaler operates on a 1D array\n",
    "# tell numpy to infer shape of 1D array with '-1' argument\n",
    "X = np.reshape(X, (N,-1)) \n",
    "X = scaler.fit_transform(X)\n",
    "# Transform back to NxCxHxW 4D tensor format\n",
    "X = np.reshape(X, (N,C,H,W))\n",
    "\n",
    "# check shape of X again\n",
    "print(f'X_train scaled:{X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9d1915d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 6, 2, 4, 4, 6, 6, 2, 2, 4, 5, 6, 0, 4, 2, 6, 4, 4, 5, 5, 6,\n",
       "       2, 5, 2, 4, 6, 2, 2, 2, 5, 5, 5, 5, 5, 5, 3, 2, 6, 6, 6, 3, 2, 6,\n",
       "       2, 2, 0, 4, 6, 2, 4, 4, 2, 6, 5, 4, 5, 5, 2, 2, 2, 5, 5, 5, 5, 2,\n",
       "       2, 6, 4, 6, 2, 2, 5, 5, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 5, 5, 3, 2,\n",
       "       7, 5, 5, 0, 5, 3, 5, 6, 6, 2, 2, 6, 5, 6, 1, 4, 1, 0, 5, 1, 3, 5,\n",
       "       3, 3, 2, 3, 6, 2, 6, 6, 6, 6, 5, 4, 4, 3, 5, 2, 6, 6, 6, 6, 4, 0,\n",
       "       3, 6, 2, 5, 0, 1, 2, 2, 6, 6, 6, 6, 2, 6, 0, 5, 6, 6, 6, 6, 5, 6,\n",
       "       6, 5, 6, 6, 2, 4, 4, 2, 6, 2, 4, 4, 6, 2, 6, 6, 6, 6, 1, 0, 0, 4,\n",
       "       0, 2, 5, 5, 1, 2, 0, 3, 6, 5, 2, 5, 0, 2, 6, 4, 4, 6, 4, 5, 4, 2,\n",
       "       6, 7, 6, 7, 3, 1, 5, 2, 6, 6, 4, 4, 6, 2, 3, 0, 5, 5, 0, 5, 2, 6,\n",
       "       5, 4, 5, 2, 3, 6, 0, 4, 3, 3, 3, 0, 0, 4, 6, 6, 6, 6, 5, 4, 0, 0,\n",
       "       5, 7, 4, 5, 3, 5, 2, 3, 6, 4, 6, 6, 2, 6, 5, 0, 2, 6, 4, 0, 0, 0,\n",
       "       5, 5, 4, 6, 0, 2, 0, 3, 3, 4, 2, 6, 4, 4, 6, 6, 6, 6, 3, 3, 5, 2,\n",
       "       5, 6, 5, 6, 5, 5, 3, 5, 3, 5, 2, 6, 4, 2, 4, 6, 0, 0, 1, 0, 5, 1,\n",
       "       2, 0, 3, 3, 2, 5, 6, 4, 4, 6, 2, 0, 3, 0, 3, 2, 6, 4, 6, 6, 6, 2,\n",
       "       2, 6, 6, 6, 0, 2, 2, 6, 0, 6, 5, 2, 3, 3, 3, 6, 0, 0, 0, 2, 5, 0,\n",
       "       6, 5, 4, 2, 6, 4, 6, 4, 2, 1, 0, 5, 5, 3, 5, 2, 4, 6, 2, 6, 4, 5,\n",
       "       2, 3, 2, 7, 4, 4, 6, 6, 5, 2, 4, 3, 2, 0, 2, 5, 5, 0, 6, 0, 5, 5,\n",
       "       3, 6, 1, 1, 5, 6, 0, 1, 2, 5, 4, 5, 6, 6, 4, 2, 6, 0, 4, 5, 5, 2,\n",
       "       5, 2, 6, 6, 5, 3, 3, 5, 0, 6, 0, 2, 3, 0, 0, 6, 6, 4, 2, 5, 5, 2,\n",
       "       5, 6, 4, 4, 6, 4, 2, 4, 5, 4, 2, 6, 2, 4, 4, 7, 1, 5, 6, 5, 5, 6,\n",
       "       6, 4, 4, 2, 2, 6, 2, 5, 5, 2, 2, 4, 6, 6, 2, 6, 6, 0, 0, 0, 5, 6,\n",
       "       0, 0, 6, 2, 7, 0, 6, 2, 5, 5, 5, 2, 6, 2, 6, 2, 2, 2, 2, 4, 2, 2,\n",
       "       4, 4, 3, 2, 2, 3, 2, 2, 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X, device=device).float()\n",
    "predictions = predict(X_tensor)\n",
    "predictions = predictions.numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5beeaa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54, 14, 103, 41, 68, 102, 128, 7]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_emotion_counts(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae490e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([42, 13, 83, 32, 53, 70, 86, 7], 386)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timestamp_counts(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "94c2642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 6, 2, 4, 4, 6, 6, 2, 2, 4, 5, 6, 0, 4, 2, 6, 4, 4, 5, 5, 6,\n",
       "       2, 2, 2, 4, 6, 2, 2, 2, 5, 5, 5, 5, 5, 5, 3, 2, 6, 6, 6, 3, 2, 2,\n",
       "       2, 2, 0, 4, 6, 2, 4, 4, 2, 6, 5, 5, 5, 5, 2, 2, 2, 5, 5, 5, 5, 2,\n",
       "       2, 6, 6, 6, 2, 2, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 5, 5, 3, 2,\n",
       "       7, 5, 5, 5, 5, 5, 5, 6, 6, 2, 2, 6, 6, 6, 1, 1, 1, 0, 5, 1, 3, 3,\n",
       "       3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 5, 4, 4, 3, 5, 2, 6, 6, 6, 6, 4, 0,\n",
       "       3, 6, 2, 5, 0, 1, 2, 2, 6, 6, 6, 6, 6, 6, 0, 5, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 2, 4, 4, 2, 2, 2, 4, 4, 6, 6, 6, 6, 6, 6, 1, 0, 0, 0,\n",
       "       0, 2, 5, 5, 1, 2, 0, 3, 6, 5, 5, 5, 0, 2, 6, 4, 4, 4, 4, 4, 4, 2,\n",
       "       6, 6, 6, 7, 3, 1, 5, 2, 6, 6, 4, 4, 6, 2, 3, 0, 5, 5, 5, 5, 2, 6,\n",
       "       5, 5, 5, 2, 3, 6, 0, 4, 3, 3, 3, 0, 0, 4, 6, 6, 6, 6, 5, 4, 0, 0,\n",
       "       5, 7, 4, 5, 5, 5, 2, 3, 6, 6, 6, 6, 6, 6, 5, 0, 2, 6, 4, 0, 0, 0,\n",
       "       5, 5, 4, 6, 0, 0, 0, 3, 3, 4, 2, 6, 4, 4, 6, 6, 6, 6, 3, 3, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 6, 4, 4, 4, 6, 0, 0, 0, 0, 5, 1,\n",
       "       2, 0, 3, 3, 2, 5, 6, 4, 4, 6, 2, 0, 0, 0, 3, 2, 6, 6, 6, 6, 6, 2,\n",
       "       2, 6, 6, 6, 0, 2, 2, 6, 6, 6, 5, 2, 3, 3, 3, 6, 0, 0, 0, 2, 5, 0,\n",
       "       6, 5, 4, 2, 6, 6, 6, 4, 2, 1, 0, 5, 5, 5, 5, 2, 4, 6, 6, 6, 4, 5,\n",
       "       2, 2, 2, 7, 4, 4, 6, 6, 5, 2, 4, 3, 2, 2, 2, 5, 5, 0, 0, 0, 5, 5,\n",
       "       3, 6, 1, 1, 5, 6, 0, 1, 2, 5, 5, 5, 6, 6, 4, 2, 6, 0, 4, 5, 5, 5,\n",
       "       5, 2, 6, 6, 5, 3, 3, 5, 0, 0, 0, 2, 3, 0, 0, 6, 6, 4, 2, 5, 5, 5,\n",
       "       5, 6, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 7, 1, 5, 5, 5, 5, 6,\n",
       "       6, 4, 4, 2, 2, 2, 2, 5, 5, 2, 2, 4, 6, 6, 6, 6, 6, 0, 0, 0, 5, 6,\n",
       "       0, 0, 6, 2, 7, 0, 6, 2, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       4, 4, 3, 2, 2, 2, 2, 2, 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soothe out the output by using a filter of length 3 and stride 1\n",
    "smooth_predictions = np.copy(predictions)\n",
    "for i in range(1, len(smooth_predictions)-2):\n",
    "    if smooth_predictions[i-1] == smooth_predictions[i+1]:\n",
    "        if smooth_predictions[i] != smooth_predictions[i-1]:\n",
    "            smooth_predictions[i] = smooth_predictions[i-1]\n",
    "smooth_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bdd14490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 14, 100, 35, 64, 113, 129, 6]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_emotion_counts(smooth_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adb1a599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32, 11, 58, 22, 37, 45, 57, 6], 268)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timestamp_counts(smooth_predictions)"
=======
   "execution_count": null,
   "id": "c9d1915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, device=device).float()\n",
    "predictions = predict(X_tensor)\n",
    "predictions[:10]"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "4e73be15",
=======
   "id": "0d4bfcb1",
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3 (ipykernel)",
=======
   "display_name": "Python 3",
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.11"
=======
   "version": "3.8.10"
>>>>>>> e53010e304a63a3e845c2ef3c8ca2d3dec6bbc78
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
